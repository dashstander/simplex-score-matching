{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29989487-c24d-4ea2-94ee-f30d215cc3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:absl:Remote TPU is not linked into jax; skipping remote TPU.\n",
      "INFO:absl:Unable to initialize backend 'tpu_driver': Could not initialize backend 'tpu_driver'\n",
      "INFO:absl:Unable to initialize backend 'cuda': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:absl:Unable to initialize backend 'rocm': module 'jaxlib.xla_extension' has no attribute 'GpuAllocatorConfig'\n",
      "INFO:absl:Unable to initialize backend 'tpu': module 'jaxlib.xla_extension' has no attribute 'get_tpu_client'\n",
      "INFO:root:Using jax backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"GEOMSTATS_BACKEND\"] = \"jax\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from geomstats.geometry.hypersphere import Hypersphere\n",
    "from geomstats.geometry.product_manifold import ProductSameManifold, ProductSameRiemannianMetric\n",
    "import geomstats.backend as gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be69170a-d16b-41df-b52b-6984a3bc0b39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0.29211295, 0.4125772 , 0.29530984],\n",
       "             [0.39183572, 0.45557895, 0.15258528],\n",
       "             [0.39755124, 0.3422433 , 0.26020542],\n",
       "             [0.29094806, 0.25036165, 0.4586903 ],\n",
       "             [0.26951846, 0.25899136, 0.47149017],\n",
       "             [0.2152559 , 0.13369545, 0.65104866],\n",
       "             [0.6735201 , 0.10467911, 0.2218008 ],\n",
       "             [0.11310774, 0.6675091 , 0.21938318],\n",
       "             [0.34312925, 0.24179856, 0.41507214],\n",
       "             [0.53123134, 0.07096099, 0.3978077 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = jax.random.PRNGKey(45)\n",
    "key, subkey = jax.random.split(key)\n",
    "\n",
    "jax.random.dirichlet(subkey, jnp.array([3, 3, 3]), (10,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8141a2c9-0b89-4261-8413-ec3eba91a6c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import choice, choices\n",
    "\n",
    "class BiGramSampler:\n",
    "    vocab = ('a', 'b', 'c')\n",
    "    \n",
    "    probs = {\n",
    "        'a': (1./3, 1./3, 1.3),\n",
    "        'b': (0.1, 0.4, 0.5),\n",
    "        'c': (0.6, 0.2, 0.2)\n",
    "    }\n",
    "    \n",
    "    def __init__(self, seq_len: int):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def _sample(self):\n",
    "        output = [choice(self.vocab)]\n",
    "        for _ in range(self.seq_len - 1):\n",
    "            output += choices(self.vocab, weights=self.probs[output[-1]])\n",
    "        return ''.join(output)\n",
    "        \n",
    "    def sample(self, n):\n",
    "        return [self._sample() for _ in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adc2342d-b0c9-4338-ad2f-ffc4ac22605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange, repeat\n",
    "from flax import linen as nn\n",
    "from flax import struct\n",
    "import jax\n",
    "from typing import Callable, Sequence\n",
    "\n",
    "\n",
    "@struct.dataclass\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    model_dim: int\n",
    "    mlp_dim: int\n",
    "    num_layers: int = 3\n",
    "    time_dim: int = 16\n",
    "    num_heads: int = 8\n",
    "    max_length: int = 512\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout_rate: float = 0.1\n",
    "    # kernel_init: Callable = nn.initializers.xavier_uniform()\n",
    "    fourier_init_std: float = 0.2\n",
    "\n",
    "\n",
    "def alpha_sigma_to_log_snr(alpha, sigma):\n",
    "    \"\"\"Returns a log snr, given the scaling factors for the clean image and for\n",
    "    the noise.\"\"\"\n",
    "    return jnp.log(alpha**2 / sigma**2)\n",
    "\n",
    "\n",
    "def t_to_alpha_sigma(t):\n",
    "    \"\"\"Returns the scaling factors for the clean image and for the noise, given\n",
    "    a timestep.\"\"\"\n",
    "    return jnp.cos(t * jnp.pi / 2), jnp.sin(t * jnp.pi / 2)\n",
    "\n",
    "\n",
    "\n",
    "class FourierFeatures(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        w = self.param(\n",
    "            'w',\n",
    "            nn.initializers.normal(stddev=self.config.fourier_init_std),\n",
    "            (self.config.time_dim // 2, x.shape[1]),\n",
    "        )\n",
    "        f = 2 * jnp.pi * x @ w.T\n",
    "        return jnp.concatenate([jnp.cos(f), jnp.sin(f)], axis=-1)\n",
    "\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, padding_mask=None, deterministic=False):\n",
    "        x = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
    "        padding_mask = None if padding_mask is None else padding_mask[:, None, None, :]\n",
    "        x = nn.MultiHeadDotProductAttention(\n",
    "            self.config.num_heads,\n",
    "            dropout_rate=self.config.attention_dropout_rate\n",
    "        )(x, x, padding_mask, deterministic=deterministic)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.LayerNorm(use_bias=False, use_scale=False)(x)\n",
    "        x = nn.Dense(self.config.mlp_dim, use_bias=False)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dense(self.config.model_dim, use_bias=False)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    config: TransformerConfig\n",
    "\n",
    "    @nn.compact    \n",
    "    def __call__(self, x, padding_mask=None, deterministic=False):\n",
    "        #x_rot = x[:, :, :self.d_rotary]\n",
    "        #x_pass = x[ :, :, self.d_rotary:]\n",
    "        #sincos = fixed_pos_embedding(x_rot, seq_dim=1)\n",
    "        #x_rot = apply_rotary_pos_emb(x_rot, sincos)\n",
    "        #x = jnp.concatenate([x_rot, x_pass], axis=-1)\n",
    "        x = x + SelfAttention(self.config)(x, padding_mask, deterministic)\n",
    "        x = x + FeedForward(self.config)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SkipBlock(nn.Module):\n",
    "    layers: Sequence[nn.Module]\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x_new = nn.Sequential(*self.layers)(x)\n",
    "        return jnp.concatenate(\n",
    "            [x_new, x], \n",
    "            dim=1\n",
    "        )\n",
    "\n",
    "\n",
    "class TransformerDiffusion(nn.Module):\n",
    "    config: TransformerConfig\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x, t, training=False):\n",
    "        \"\"\"\n",
    "        x.shape = \n",
    "        \"\"\"\n",
    "        deterministic = not training\n",
    "        #x = normalize_probabilities(x)\n",
    "        #x_init = nn.Dense(self.config.embed_dim)(x)\n",
    "        log_snr = alpha_sigma_to_log_snr(*t_to_alpha_sigma(t))\n",
    "        timestep_embed = FourierFeatures(self.config)(log_snr[:, None])\n",
    "        te_planes = jnp.tile(timestep_embed[:, None], (1, self.config.max_length, 1))\n",
    "        x = jnp.concatenate([x, te_planes], axis=-1)\n",
    "        x = FeedForward(self.config)(x)\n",
    "        trans_x = nn.Sequential([\n",
    "            TransformerLayer(self.config) for _ in range(self.config.num_layers)\n",
    "        ])(x, None, deterministic=deterministic)        \n",
    "        x = x + jnp.sqrt(2) * trans_x\n",
    "        x = nn.Dense(self.config.vocab_size)(x)\n",
    "        x_final = x - jnp.mean(x, axis=-1, keepdims=True)\n",
    "        x_final = x_final / jnp.var(x_final, axis=-1, keepdims=True)\n",
    "        return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "546cfd60-e4cf-4bfc-862f-d0f86ed0c3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class TransformerConfig:\n",
    "    vocab_size: int\n",
    "    embed_dim: int\n",
    "    model_dim: int\n",
    "    mlp_dim: int\n",
    "    num_layers: int = 3\n",
    "    time_dim: int = 16\n",
    "    num_heads: int = 8\n",
    "    max_length: int = 512\n",
    "    dropout_rate: float = 0.1\n",
    "    attention_dropout_rate: float = 0.1\n",
    "    # kernel_init: Callable = nn.initializers.xavier_uniform()\n",
    "    fourier_init_std: float = 0.2\n",
    "\"\"\"\n",
    "\n",
    "config = TransformerConfig(\n",
    "    vocab_size=3,\n",
    "    model_dim=12,\n",
    "    mlp_dim=24,\n",
    "    num_layers =1,\n",
    "    time_dim=4,\n",
    "    num_heads=1,\n",
    "    max_length= 24,\n",
    "    dropout_rate= 0.1,\n",
    "    attention_dropout_rate = 0.1,\n",
    "    # kernel_init: Callable = nn.initializers.xavier_uniform()\n",
    "    fourier_init_std = 0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc364b7c-5653-4527-94ed-78997405a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramSampler:\n",
    "    vocab = ('a', 'b', 'c')\n",
    "    \n",
    "    probs = {\n",
    "        'a': (1./3, 1./3, 1.3),\n",
    "        'b': (0.1, 0.4, 0.5),\n",
    "        'c': (0.6, 0.2, 0.2)\n",
    "    }\n",
    "\n",
    "    vectors = {\n",
    "        'a': jnp.array([1., 0., 0.]),\n",
    "        'b': jnp.array([0., 1., 0.]),\n",
    "        'c': jnp.array([0., 0., 1.])\n",
    "    }\n",
    "    \n",
    "    def __init__(self, seq_len: int):\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "    def _sample(self):\n",
    "        output = [choice(self.vocab)]\n",
    "        for _ in range(self.seq_len - 1):\n",
    "            output += choices(self.vocab, weights=self.probs[output[-1]])\n",
    "        return ''.join(output)\n",
    "        \n",
    "    def sample(self, n: int):\n",
    "        return [self._sample() for _ in range(n)]\n",
    "\n",
    "    def to_jax(self, s):\n",
    "        return jnp.stack([self.vectors[char] for char in s])\n",
    "\n",
    "    def make_batch(self, n: int):\n",
    "        tensors = [\n",
    "            self.to_jax(self._sample()) for _ in range(n)\n",
    "        ]\n",
    "        return jnp.stack(tensors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a796e078-a3f8-4bfc-8b21-6c32d61b3033",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = BiGramSampler(10)\n",
    "\n",
    "batch = sampler.make_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "09bafce2-1261-44ff-bc3a-9f35cbccdf1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([[0., 0., 1.],\n",
       "             [1., 0., 0.],\n",
       "             [0., 0., 1.],\n",
       "             [0., 1., 0.],\n",
       "             [1., 0., 0.],\n",
       "             [1., 0., 0.],\n",
       "             [1., 0., 0.],\n",
       "             [0., 0., 1.],\n",
       "             [1., 0., 0.],\n",
       "             [0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7880e-d90c-4f8d-b0d0-e3c2b0e9d143",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ssm]",
   "language": "python",
   "name": "conda-env-ssm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
