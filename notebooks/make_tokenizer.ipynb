{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22f62947-3cfd-40a2-ae6f-6168b8ec72b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "/Users/dashiell/miniconda3/envs/ssm/lib/python3.9/site-packages/jax/_src/lib/__init__.py:34: UserWarning: JAX on Mac ARM machines is experimental and minimally tested. Please see https://github.com/google/jax/issues/5501 in the event of problems.\n",
      "  warnings.warn(\"JAX on Mac ARM machines is experimental and minimally tested. \"\n"
     ]
    }
   ],
   "source": [
    "import jax.random as jrand\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import nump as np\n",
    "from tokenizers import normalizers, pre_tokenizers\n",
    "from tokenizers.normalizers import NFD, StripAccents, Lowercase\n",
    "from tokenizers.pre_tokenizers import Digits, Whitespace\n",
    "\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "\n",
    "pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Digits(individual_digits=True)])\n",
    "normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])\n",
    "\n",
    "\n",
    "files = [f'/Users/dashiell/workspace/wikitext/wikitext-103-raw/wiki.{split}.raw' for split in ['train', 'test', 'valid']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b66d91ae-8976-4b0a-bf9b-fab9a22203f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\"], vocab_size=8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90ceaf25-9c4c-41ab-9c86-1ea538cf77a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = pre_tokenizer\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "619164d1-5400-48f5-ad50-90641c61cfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tokenizer/tokenizer-wiki8192.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "436219c6-3d68-4fc7-880b-99cb09c93398",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(\"tokenizer/tokenizer-wiki8192.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c38f458-0cbc-4f09-8cde-31519f56b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = tokenizer.encode(\"Hello, y'all! How are you üòÅ ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be59b226-c179-4b91-a02a-dfd11a79c451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.token_to_id('[PAD]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfd7ac3a-cf00-479d-a2ba-59c8d4806b7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hel', 'lo', ',', 'y', \"'\", 'all', '!', 'How', 'are', 'you', '[UNK]', '?']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef2a6fe-cdf1-4e68-be4b-7e30c6977b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikipedia (/Users/dashiell/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab9b348c33f412a9aebadd84165b30b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"wikipedia\", \"20220301.en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccd9dc30-d9cd-4bb7-aef2-51f4eb031355",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11450, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data = data.rename_column('text', 'article')\n",
    "\n",
    "tokenizer.encode(data['train'][0]['article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5cc692d2-d816-43b0-97c1-1820960efea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b59829b-d207-41e3-ba04-f2809dd10c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def tokenize_and_split(batch):\n",
    "    seq_len = 512\n",
    "    texts = '[SEP]'.join(batch['text'])\n",
    "    cls_token = tokenizer.token_to_id('[CLS]')\n",
    "    pad_token = tokenizer.token_to_id('[PAD]')\n",
    "    token_ids = np.array(tokenizer.encode(texts).ids)\n",
    "    leftover = token_ids.shape[0] % (seq_len - 1)\n",
    "    pads = np.full(((seq_len - 1) - leftover,), pad_token)\n",
    "    split_tokens = np.append(token_ids, pads).reshape((-1, seq_len - 1))\n",
    "    num_splits = split_tokens.shape[0]\n",
    "    extra_cls_tokens = np.full((num_splits, 1), cls_token)\n",
    "    fully_tokenized = np.hstack([extra_cls_tokens, split_tokens])\n",
    "    return np.vsplit(fully_tokenized, num_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89aeb32-c68d-4b4f-89d2-80396a9b5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_chunk(index, path, data):\n",
    "    tokens = tokenize_and_split(data)\n",
    "    df = pl.DataFrame({'tokens': tokens})\n",
    "    df.write_parquet(path / f'chunk_{index}.parquet')\n",
    "    \n",
    "futures = []\n",
    "chunk_size = 10_000\n",
    "num_rows = data['train'].num_rows\n",
    "fp = Path('/Users/dstander/workspace/simplex-score-matching/data/train')\n",
    "with ThreadPoolExecutor(max_workers=32) as executor:\n",
    "    for chunk_no, i in enumerate(range(0, num_rows, chunk_size)):\n",
    "        j = min(i + chunk_size, num_rows)\n",
    "        chunk = data['train'][i:j]\n",
    "        futures.append(executor.submit(save_chunk, chunk_no, fp, chunk))\n",
    "\n",
    "total_rows = 0\n",
    "for i, fut in as_completed(futures):\n",
    "    try:\n",
    "        rows = fut.result()\n",
    "    except Exception:\n",
    "        print(f'Chunk {i} failed')\n",
    "        continue\n",
    "    total_rows += rows\n",
    "    if i // 50 == 0:\n",
    "        print(f'Saved {total_rows} records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1557890f-272d-4cec-a49d-4d381399ca7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tokenize_and_split(data['train'][0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5c2a8e9d-75fc-4205-b6ad-897ecf34afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 100_000, 10_000):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b4d1ffd-5824-4250-9326-b2f642f6aecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5bc001ce5f45e38e8d08f478ab8a47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6458670 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "for i in tqdm(range(data['train'].num_rows)):\n",
    "    if i // 1000 == 0:\n",
    "        print(data['train'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f545b0df-d5cc-400d-80ca-43a06f96bc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.arange(25).reshape((5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8495efa-ba0a-45c8-96fd-acd424dccc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45],\n",
       "       [45],\n",
       "       [45],\n",
       "       [45],\n",
       "       [45]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.full((5, 1), 45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bc848b-ae71-4cb9-878c-6ee577c6afe0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
