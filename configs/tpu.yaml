description: "Small model to test, runs on a v2-8 TPU"

gcs:
  project: simplex-score-matching

data:
    epochs: 20
    dataset_path: "/home/dashiell/wiki_tokens_512.npy"
    batch_size: 64
    eval_batch_size: 32
    seq_len: 512
    val_size: 2048
    min_init_prob_concentration: 0.05
    max_init_prob_concentration: 0.9

tokenizer:
    path: "tokenizer/tokenizer-wiki8192.json"
    vocab_size: 8192
  
sde:
  start_time: 0
  end_time: 10

model:
    type: transformer
    num_layers: 3
    num_heads: 8
    embed_dim: 512
    init_dim: 512
    model_dim: 512
    mlp_dim: 2048
    time_dim: 16
    dropout: 0.1
    attention_dropout: 0.1

optimizer:
    algo: AdamW
    init_lr: 0.00000002
    peak_lr: 0.1
    lr: 0.0003
    warmup_steps: 1000
    decay_steps: 10000
    grad_accum: 8
    grad_clip: 2.0
