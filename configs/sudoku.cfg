[data]
seq_len = 81
vocab_size = 9
batch_size = 512


[model]
vocab_size = ${data.vocab_size}
num_layers = 2
num_heads = 8
embed_dim = 128
model_dim = 256
mlp_dim = 128
time_dim = 16
dropout = 0.1
attention_dropout = 0.1

[sde]
num_steps = 100


[optimizer]
algo = "AdamW"
init_lr = 0.0000002
peak_lr = 0.0006
lr = 0.0003
warmup_steps = 500
decay_steps = 50000
grad_clip = 2.0